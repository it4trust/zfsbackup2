#!/bin/bash

# ZFS Backup Script v2.0
# Automatisierte inkrementelle ZFS-Backups auf wechselnde externe Festplatten
# Kompatibel mit Proxmox 8 / Debian 12
# Neu: Robuste Disk-Erkennung, Disk-Rotation Tracking, Graceful Degradation

set -euo pipefail

# Globale Variablen
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CONFIG_FILE="${SCRIPT_DIR}/zfs-backup2-skript.conf"
LOCKFILE="/var/run/zfs_backup.lock"
LOGFILE="/var/log/zfs-backup2.log"
VERBOSE=false
DRY_RUN=false

# Globale Status-Variablen
BACKUP_SUCCESS_COUNT=0
BACKUP_FAILURE_COUNT=0
CURRENT_DISK_ID=""
CURRENT_DISK_PATH=""

# Logging-Funktion
log() {
    local level="$1"
    shift
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] [$level] $*" | tee -a "$LOGFILE"
}

# Verbose Logging
vlog() {
    if [[ "$VERBOSE" == true ]]; then
        log "DEBUG" "$@"
    fi
}

# Fehlerbehandlung
error_exit() {
    log "ERROR" "$1"
    cleanup
    exit 1
}

# Cleanup-Funktion
cleanup() {
    if [[ -f "$LOCKFILE" ]]; then
        rm -f "$LOCKFILE"
        vlog "Lockfile entfernt"
    fi
    
    # Export des Backup-Pools falls importiert
    if [[ -n "${BACKUP_POOL:-}" ]] && zpool list "$BACKUP_POOL" >/dev/null 2>&1; then
        log "INFO" "Exportiere Backup-Pool: $BACKUP_POOL"
        if [[ "$DRY_RUN" == false ]]; then
            zpool export "$BACKUP_POOL" || log "WARN" "Fehler beim Export von $BACKUP_POOL"
        fi
    fi
}

# Signal Handler
trap cleanup EXIT INT TERM

# Konfiguration laden
load_config() {
    if [[ ! -f "$CONFIG_FILE" ]]; then
        error_exit "Konfigurationsdatei nicht gefunden: $CONFIG_FILE"
    fi
    
    source "$CONFIG_FILE"
    
    # Validierung der Pflichtparameter
    if [[ -z "${DATASETS:-}" ]]; then
        error_exit "DATASETS nicht in Konfiguration definiert"
    fi
    
    if [[ -z "${BACKUP_DISK_IDS:-}" ]]; then
        error_exit "BACKUP_DISK_IDS nicht in Konfiguration definiert"
    fi
    
    if [[ -z "${BACKUP_POOL:-}" ]]; then
        error_exit "BACKUP_POOL nicht in Konfiguration definiert"
    fi
    
    # Standardwerte setzen
    MIN_FREE_SPACE_GB=${MIN_FREE_SPACE_GB:-10}
    FULL_BACKUP_ON_NO_COMMON=${FULL_BACKUP_ON_NO_COMMON:-true}
    SYSTEM_UPDATE_ENABLED=${SYSTEM_UPDATE_ENABLED:-false}
    CHECKMK_ENABLED=${CHECKMK_ENABLED:-true}
    CHECKMK_PIGGYBACK_DIR=${CHECKMK_PIGGYBACK_DIR:-"/var/spool/check_mk_agent/piggyback"}
    CHECKMK_DETAILED_SERVICES=${CHECKMK_DETAILED_SERVICES:-true}
    
    # Neue Parameter
    MAX_DISK_USAGE_HOURS=${MAX_DISK_USAGE_HOURS:-168}
    DISK_ROTATION_DIR=${DISK_ROTATION_DIR:-"/var/lib/zfs-backup"}
    ENABLE_DISK_FALLBACK=${ENABLE_DISK_FALLBACK:-true}
    DISK_DETECTION_TIMEOUT=${DISK_DETECTION_TIMEOUT:-30}
    DISK_IMPORT_RETRIES=${DISK_IMPORT_RETRIES:-3}
    DISK_IMPORT_RETRY_DELAY=${DISK_IMPORT_RETRY_DELAY:-5}
    CONTINUE_ON_DATASET_FAILURE=${CONTINUE_ON_DATASET_FAILURE:-true}
    MAX_DATASET_FAILURES=${MAX_DATASET_FAILURES:-0}
    
    vlog "Konfiguration geladen:"
    vlog "- DATASETS: $DATASETS"
    vlog "- BACKUP_POOL: $BACKUP_POOL"
    vlog "- MIN_FREE_SPACE_GB: $MIN_FREE_SPACE_GB"
    vlog "- CONTINUE_ON_DATASET_FAILURE: $CONTINUE_ON_DATASET_FAILURE"
    vlog "- MAX_DISK_USAGE_HOURS: $MAX_DISK_USAGE_HOURS"
}

# Lockfile prüfen und erstellen
acquire_lock() {
    if [[ -f "$LOCKFILE" ]]; then
        local lock_age=$(($(date +%s) - $(stat -c %Y "$LOCKFILE")))
        if [[ $lock_age -gt ${MAX_LOCK_AGE:-3600} ]]; then
            log "WARN" "Verwaistes Lockfile gefunden (Alter: ${lock_age}s), wird entfernt"
            rm -f "$LOCKFILE"
        else
            error_exit "Backup bereits aktiv (Lockfile: $LOCKFILE)"
        fi
    fi
    
    echo $$ > "$LOCKFILE"
    vlog "Lockfile erstellt: $LOCKFILE"
}

# Disk-Rotation Verzeichnis erstellen
ensure_disk_rotation_dir() {
    if [[ ! -d "$DISK_ROTATION_DIR" ]]; then
        mkdir -p "$DISK_ROTATION_DIR"
        vlog "Disk-Rotation Verzeichnis erstellt: $DISK_ROTATION_DIR"
    fi
}

# Disk-Nutzung tracken
track_disk_usage() {
    local disk_id="$1"
    local action="$2"  # "start" oder "end"
    
    local tracking_file="$DISK_ROTATION_DIR/${disk_id}.usage"
    local timestamp=$(date +%s)
    
    case "$action" in
        "start")
            echo "start_time=$timestamp" > "$tracking_file"
            echo "disk_id=$disk_id" >> "$tracking_file"
            vlog "Disk-Nutzung gestartet: $disk_id"
            ;;
        "end")
            if [[ -f "$tracking_file" ]]; then
                local start_time=$(grep "^start_time=" "$tracking_file" | cut -d= -f2)
                local duration_hours=$(( (timestamp - start_time) / 3600 ))
                echo "end_time=$timestamp" >> "$tracking_file"
                echo "duration_hours=$duration_hours" >> "$tracking_file"
                vlog "Disk-Nutzung beendet: $disk_id (Dauer: ${duration_hours}h)"
            fi
            ;;
    esac
}

# Disk-Nutzungsdauer prüfen
check_disk_usage_duration() {
    local disk_id="$1"
    
    if [[ "$MAX_DISK_USAGE_HOURS" -eq 0 ]]; then
        return 0  # Tracking deaktiviert
    fi
    
    local tracking_file="$DISK_ROTATION_DIR/${disk_id}.usage"
    
    if [[ ! -f "$tracking_file" ]]; then
        return 0  # Neue Disk
    fi
    
    # Prüfe ob Disk noch in Verwendung ist
    if ! grep -q "^end_time=" "$tracking_file"; then
        # Disk ist noch in Verwendung, prüfe Dauer
        local start_time=$(grep "^start_time=" "$tracking_file" | cut -d= -f2 2>/dev/null || echo "0")
        local current_time=$(date +%s)
        local usage_hours=$(( (current_time - start_time) / 3600 ))
        
        if [[ $usage_hours -gt $MAX_DISK_USAGE_HOURS ]]; then
            log "WARN" "Disk $disk_id wird seit ${usage_hours}h verwendet (Maximum: ${MAX_DISK_USAGE_HOURS}h)"
            return 1
        fi
    fi
    
    return 0
}

# Disk-Identifikation mit Fallback-Strategien
identify_disk_robust() {
    local disk_id="$1"
    local timeout_end=$(($(date +%s) + DISK_DETECTION_TIMEOUT))
    
    vlog "Suche Disk mit ID: $disk_id (Timeout: ${DISK_DETECTION_TIMEOUT}s)"
    
    while [[ $(date +%s) -lt $timeout_end ]]; do
        local disk_path=""
        
        # Strategie 1: WWN-basierte Erkennung
        if [[ -e "/dev/disk/by-id/wwn-$disk_id" ]]; then
            disk_path="/dev/disk/by-id/wwn-$disk_id"
            vlog "Disk gefunden über WWN: $disk_path"
        
        # Strategie 2: ATA-Serial-basierte Erkennung
        elif ls /dev/disk/by-id/ata-*$disk_id* >/dev/null 2>&1; then
            disk_path=$(ls /dev/disk/by-id/ata-*$disk_id* | head -1)
            vlog "Disk gefunden über ATA-Serial: $disk_path"
        
        # Strategie 3: SCSI-ID-basierte Erkennung
        elif ls /dev/disk/by-id/scsi-*$disk_id* >/dev/null 2>&1; then
            disk_path=$(ls /dev/disk/by-id/scsi-*$disk_id* | head -1)
            vlog "Disk gefunden über SCSI-ID: $disk_path"
        
        # Strategie 4: USB-Serial (für USB-Disks)
        elif ls /dev/disk/by-id/usb-*$disk_id* >/dev/null 2>&1; then
            disk_path=$(ls /dev/disk/by-id/usb-*$disk_id* | head -1)
            vlog "Disk gefunden über USB-Serial: $disk_path"
        
        # Fallback-Strategie: Alle verfügbaren Disks scannen
        elif [[ "$ENABLE_DISK_FALLBACK" == true ]]; then
            vlog "Fallback: Scanne alle verfügbaren Block-Geräte..."
            for dev in /dev/disk/by-id/*; do
                if [[ -e "$dev" ]] && [[ "$dev" == *"$disk_id"* ]]; then
                    disk_path="$dev"
                    vlog "Disk gefunden über Fallback-Scan: $disk_path"
                    break
                fi
            done
        fi
        
        if [[ -n "$disk_path" ]]; then
            # Prüfe ob Disk tatsächlich verfügbar ist
            if [[ -b "$disk_path" ]]; then
                echo "$disk_path"
                return 0
            else
                vlog "Disk-Pfad existiert, aber ist kein Block-Gerät: $disk_path"
            fi
        fi
        
        sleep 1
    done
    
    return 1
}

# Backup-Pool mit Retry-Logik importieren
import_backup_pool_robust() {
    local disk_path="$1"
    local disk_dir="$(dirname "$disk_path")"
    local retry_count=0
    
    while [[ $retry_count -lt $DISK_IMPORT_RETRIES ]]; do
        log "INFO" "Importiere Backup-Pool: $BACKUP_POOL (Versuch $((retry_count + 1))/$DISK_IMPORT_RETRIES)"
        
        if [[ "$DRY_RUN" == false ]]; then
            if zpool import -d "$disk_dir" "$BACKUP_POOL" 2>/dev/null; then
                log "INFO" "Backup-Pool erfolgreich importiert"
                return 0
            else
                log "WARN" "Import-Versuch $((retry_count + 1)) fehlgeschlagen"
                retry_count=$((retry_count + 1))
                
                if [[ $retry_count -lt $DISK_IMPORT_RETRIES ]]; then
                    log "INFO" "Warte ${DISK_IMPORT_RETRY_DELAY}s vor nächstem Versuch..."
                    sleep "$DISK_IMPORT_RETRY_DELAY"
                fi
            fi
        else
            log "INFO" "Backup-Pool Import (Dry-Run)"
            return 0
        fi
    done
    
    return 1
}

# Backup-Festplatte identifizieren und importieren
identify_and_import_backup_disk() {
    log "INFO" "Suche nach Backup-Festplatte..."
    
    ensure_disk_rotation_dir
    
    local found_disk=""
    local found_disk_id=""
    IFS=',' read -ra DISK_IDS <<< "$BACKUP_DISK_IDS"
    
    # Priorisierte Suche: Erste verfügbare Disk aus der Liste verwenden
    for disk_id in "${DISK_IDS[@]}"; do
        disk_id=$(echo "$disk_id" | xargs) # Whitespace entfernen
        
        local disk_path=$(identify_disk_robust "$disk_id")
        
        if [[ -n "$disk_path" ]]; then
            found_disk="$disk_path"
            found_disk_id="$disk_id"
            log "INFO" "Backup-Festplatte gefunden: $found_disk (ID: $disk_id)"
            
            # Prüfe Disk-Nutzungsdauer
            if ! check_disk_usage_duration "$disk_id"; then
                log "WARN" "Disk-Rotation Warnung für $disk_id - sollte gewechselt werden"
            fi
            
            break
        else
            vlog "Disk nicht gefunden: $disk_id"
        fi
    done
    
    if [[ -z "$found_disk" ]]; then
        error_exit "Keine gültige Backup-Festplatte gefunden. Erwartete IDs: $BACKUP_DISK_IDS"
    fi
    
    # Globale Variablen setzen
    CURRENT_DISK_ID="$found_disk_id"
    CURRENT_DISK_PATH="$found_disk"
    
    # Disk-Nutzung tracken
    track_disk_usage "$found_disk_id" "start"
    
    # Prüfen ob Pool bereits importiert ist
    if zpool list "$BACKUP_POOL" >/dev/null 2>&1; then
        log "INFO" "Backup-Pool ist bereits importiert: $BACKUP_POOL"
        return
    fi
    
    # Pool mit Retry-Logik importieren
    if ! import_backup_pool_robust "$found_disk"; then
        error_exit "Fehler beim Import des Backup-Pools nach $DISK_IMPORT_RETRIES Versuchen"
    fi
}

# Speicherplatz prüfen
check_free_space() {
    log "INFO" "Prüfe verfügbaren Speicherplatz auf $BACKUP_POOL"
    
    local available_gb
    if [[ "$DRY_RUN" == false ]]; then
        available_gb=$(zfs list -H -o avail "$BACKUP_POOL" | numfmt --from=iec --to-unit=1G)
    else
        available_gb=100 # Dummy-Wert für Dry-Run
    fi
    
    if (( $(echo "$available_gb < $MIN_FREE_SPACE_GB" | bc -l) )); then
        error_exit "Zu wenig freier Speicherplatz: ${available_gb}GB < ${MIN_FREE_SPACE_GB}GB"
    fi
    
    log "INFO" "Verfügbarer Speicherplatz: ${available_gb}GB (Minimum: ${MIN_FREE_SPACE_GB}GB)"
}

# Dataset-Mapping parsen
parse_dataset_mapping() {
    local dataset_config="$1"
    local source_dataset=""
    local target_dataset=""
    
    if [[ "$dataset_config" == *":"* ]]; then
        # Format: "source:target"
        source_dataset="${dataset_config%%:*}"
        target_dataset="${dataset_config##*:}"
        
        # Wenn target_dataset nicht mit dem BACKUP_POOL beginnt, ist es relativ
        if [[ "$target_dataset" != "$BACKUP_POOL"* ]]; then
            target_dataset="${BACKUP_POOL}/${target_dataset}"
        fi
    else
        # Traditionelles Format: "rpool/data" -> "backuppool/rpool/data"
        source_dataset="$dataset_config"
        target_dataset="${BACKUP_POOL}/${source_dataset}"
    fi
    
    echo "$source_dataset|$target_dataset"
}

# Gemeinsamen Snapshot finden
find_common_snapshot() {
    local source_dataset="$1"
    local target_dataset="$2"
    
    vlog "Suche gemeinsamen Snapshot zwischen $source_dataset und $target_dataset" >&2
    
    # Prüfe zuerst ob das Ziel-Dataset überhaupt existiert
    if [[ "$DRY_RUN" == false ]]; then
        if ! zfs list "$target_dataset" >/dev/null 2>&1; then
            vlog "Ziel-Dataset $target_dataset existiert nicht - kein gemeinsamer Snapshot möglich" >&2
            echo ""
            return
        fi
    fi
    
    # Alle zfs-auto-snap_daily Snapshots der Quelle holen
    local source_snapshots
    if [[ "$DRY_RUN" == false ]]; then
        source_snapshots=$(zfs list -H -t snapshot -o name "$source_dataset" 2>/dev/null | grep "${SNAPSHOT_PREFIX:-zfs-auto-snap_daily}" | sort -r || true)
    else
        source_snapshots="$source_dataset@zfs-auto-snap_daily-2024-01-20-00h00"
    fi
    
    if [[ -z "$source_snapshots" ]]; then
        vlog "Keine ${SNAPSHOT_PREFIX:-zfs-auto-snap_daily} Snapshots in $source_dataset gefunden" >&2
        echo ""
        return
    fi
    
    # Alle Snapshots des Ziels holen
    local target_snapshots
    if [[ "$DRY_RUN" == false ]]; then
        target_snapshots=$(zfs list -H -t snapshot -o name "$target_dataset" 2>/dev/null | grep "${SNAPSHOT_PREFIX:-zfs-auto-snap_daily}" || true)
    else
        target_snapshots="$target_dataset@zfs-auto-snap_daily-2024-01-20-00h00"
    fi
    
    if [[ -z "$target_snapshots" ]]; then
        vlog "Keine ${SNAPSHOT_PREFIX:-zfs-auto-snap_daily} Snapshots in $target_dataset gefunden" >&2
        echo ""
        return
    fi
    
    # Prüfe jeden Quell-Snapshot auf Existenz im Ziel
    while IFS= read -r source_snapshot; do
        [[ -z "$source_snapshot" ]] && continue
        
        local snap_name="${source_snapshot##*@}"
        local target_snapshot="${target_dataset}@${snap_name}"
        
        # Prüfe ob entsprechender Snapshot im Ziel existiert
        if echo "$target_snapshots" | grep -q "^${target_snapshot}$"; then
            vlog "Gemeinsamer Snapshot gefunden: $source_snapshot" >&2
            echo "$source_snapshot"
            return
        fi
    done <<< "$source_snapshots"
    
    vlog "Kein gemeinsamer Snapshot gefunden" >&2
    echo ""
}

# Neuesten Snapshot finden
find_latest_snapshot() {
    local dataset="$1"
    
    if [[ "$DRY_RUN" == false ]]; then
        zfs list -H -t snapshot -o name "$dataset" 2>/dev/null | grep "${SNAPSHOT_PREFIX:-zfs-auto-snap_daily}" | sort -r | head -1 || true
    else
        echo "$dataset@zfs-auto-snap_daily-2024-01-21-00h00"
    fi
}

# Snapshot übertragen
transfer_snapshot() {
    local source_dataset="$1"
    local target_dataset="$2"
    local common_snapshot="$3"
    local latest_snapshot="$4"
    
    log "INFO" "Übertrage Snapshot: $latest_snapshot"
    
    if [[ -n "$common_snapshot" ]]; then
        # Inkrementelle Übertragung
        log "INFO" "Inkrementelle Übertragung von $common_snapshot zu $latest_snapshot"
        vlog "Befehl: zfs send -I $common_snapshot $latest_snapshot | zfs receive -F $target_dataset"
        
        if [[ "$DRY_RUN" == false ]]; then
            if ! zfs send -I "$common_snapshot" "$latest_snapshot" | zfs receive -F "$target_dataset"; then
                log "ERROR" "Fehler bei inkrementeller Übertragung"
                return 1
            fi
        fi
    else
        # Vollständige Übertragung
        if [[ "$FULL_BACKUP_ON_NO_COMMON" == true ]]; then
            log "INFO" "Vollständige Übertragung: $latest_snapshot"
            vlog "Befehl: zfs send $latest_snapshot | zfs receive -F $target_dataset"
            
            if [[ "$DRY_RUN" == false ]]; then
                if ! zfs send "$latest_snapshot" | zfs receive -F "$target_dataset"; then
                    log "ERROR" "Fehler bei vollständiger Übertragung"
                    return 1
                fi
            fi
        else
            log "WARN" "Kein gemeinsamer Snapshot gefunden und FULL_BACKUP_ON_NO_COMMON=false"
            return 1
        fi
    fi
    
    return 0
}

# Alte Snapshots bereinigen (alle zfs-auto-snap Typen)
cleanup_old_snapshots() {
    local source_dataset="$1"
    local target_dataset="$2"
    
    log "INFO" "Bereinige alte Snapshots in $target_dataset"
    
    # Alle zfs-auto-snap Snapshot-Typen definieren
    local snap_types=("daily" "hourly" "weekly" "monthly" "frequent")
    
    for snap_type in "${snap_types[@]}"; do
        vlog "Bereinige $snap_type Snapshots..."
        
        # Alle zfs-auto-snap_$snap_type Snapshots der Quelle holen
        local source_snapshots
        if [[ "$DRY_RUN" == false ]]; then
            source_snapshots=$(zfs list -H -t snapshot -o name "$source_dataset" 2>/dev/null | grep "zfs-auto-snap_${snap_type}" | sed "s/.*@//" || true)
        else
            source_snapshots="zfs-auto-snap_${snap_type}-2024-01-21-00h00"
        fi
        
        # Alle zfs-auto-snap_$snap_type Snapshots des Ziels holen
        local target_snapshots
        if [[ "$DRY_RUN" == false ]]; then
            target_snapshots=$(zfs list -H -t snapshot -o name "$target_dataset" 2>/dev/null | grep "zfs-auto-snap_${snap_type}" || true)
        else
            target_snapshots="$target_dataset@zfs-auto-snap_${snap_type}-2024-01-20-00h00"
        fi
        
        if [[ -z "$target_snapshots" ]]; then
            vlog "Keine $snap_type Snapshots im Ziel gefunden"
            continue
        fi
        
        # Prüfe jeden Ziel-Snapshot
        while IFS= read -r target_snapshot; do
            [[ -z "$target_snapshot" ]] && continue
            
            local snap_name="${target_snapshot##*@}"
            
            # Prüfe ob Snapshot noch in Quelle existiert
            if ! echo "$source_snapshots" | grep -q "^$snap_name$"; then
                log "INFO" "Lösche verwaisten $snap_type Snapshot: $target_snapshot"
                if [[ "$DRY_RUN" == false ]]; then
                    zfs destroy "$target_snapshot" || log "WARN" "Fehler beim Löschen von $target_snapshot"
                fi
            else
                vlog "Behalte $snap_type Snapshot: $target_snapshot (existiert noch in Quelle)"
            fi
        done <<< "$target_snap

#!/bin/bash

# ZFS Backup Script v2.0
# Automatisierte inkrementelle ZFS-Backups auf wechselnde externe Festplatten
# Kompatibel mit Proxmox 8 / Debian 12
# Neu: Robuste Disk-Erkennung, Disk-Rotation Tracking, Graceful Degradation

set -euo pipefail

# Globale Variablen
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CONFIG_FILE="${SCRIPT_DIR}/zfs-backup2-skript.conf"
LOCKFILE="/var/run/zfs_backup.lock"
LOGFILE="/var/log/zfs-backup2.log"
VERBOSE=false
DRY_RUN=false

# Globale Status-Variablen
BACKUP_SUCCESS_COUNT=0
BACKUP_FAILURE_COUNT=0
CURRENT_DISK_ID=""
CURRENT_DISK_PATH=""

# Logging-Funktion
log() {
    local level="$1"
    shift
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] [$level] $*" | tee -a "$LOGFILE"
}

# Verbose Logging
vlog() {
    if [[ "$VERBOSE" == true ]]; then
        log "DEBUG" "$@"
    fi
}

# Fehlerbehandlung
error_exit() {
    log "ERROR" "$1"
    cleanup
    exit 1
}

# Cleanup-Funktion
cleanup() {
    if [[ -f "$LOCKFILE" ]]; then
        rm -f "$LOCKFILE"
        vlog "Lockfile entfernt"
    fi
    
    # Export des Backup-Pools falls importiert
    if [[ -n "${BACKUP_POOL:-}" ]] && zpool list "$BACKUP_POOL" >/dev/null 2>&1; then
        log "INFO" "Exportiere Backup-Pool: $BACKUP_POOL"
        if [[ "$DRY_RUN" == false ]]; then
            zpool export "$BACKUP_POOL" || log "WARN" "Fehler beim Export von $BACKUP_POOL"
        fi
    fi
}

# Signal Handler
trap cleanup EXIT INT TERM

# Konfiguration laden
load_config() {
    if [[ ! -f "$CONFIG_FILE" ]]; then
        error_exit "Konfigurationsdatei nicht gefunden: $CONFIG_FILE"
    fi
    
    source "$CONFIG_FILE"
    
    # Validierung der Pflichtparameter
    if [[ -z "${DATASETS:-}" ]]; then
        error_exit "DATASETS nicht in Konfiguration definiert"
    fi
    
    if [[ -z "${BACKUP_DISK_IDS:-}" ]]; then
        error_exit "BACKUP_DISK_IDS nicht in Konfiguration definiert"
    fi
    
    if [[ -z "${BACKUP_POOL:-}" ]]; then
        error_exit "BACKUP_POOL nicht in Konfiguration definiert"
    fi
    
    # Standardwerte setzen
    MIN_FREE_SPACE_GB=${MIN_FREE_SPACE_GB:-10}
    FULL_BACKUP_ON_NO_COMMON=${FULL_BACKUP_ON_NO_COMMON:-true}
    SYSTEM_UPDATE_ENABLED=${SYSTEM_UPDATE_ENABLED:-false}
    CHECKMK_ENABLED=${CHECKMK_ENABLED:-true}
    CHECKMK_PIGGYBACK_DIR=${CHECKMK_PIGGYBACK_DIR:-"/var/spool/check_mk_agent/piggyback"}
    CHECKMK_DETAILED_SERVICES=${CHECKMK_DETAILED_SERVICES:-true}
    
    # Neue Parameter
    MAX_DISK_USAGE_HOURS=${MAX_DISK_USAGE_HOURS:-168}
    DISK_ROTATION_DIR=${DISK_ROTATION_DIR:-"/var/lib/zfs-backup"}
    ENABLE_DISK_FALLBACK=${ENABLE_DISK_FALLBACK:-true}
    DISK_DETECTION_TIMEOUT=${DISK_DETECTION_TIMEOUT:-30}
    DISK_IMPORT_RETRIES=${DISK_IMPORT_RETRIES:-3}
    DISK_IMPORT_RETRY_DELAY=${DISK_IMPORT_RETRY_DELAY:-5}
    CONTINUE_ON_DATASET_FAILURE=${CONTINUE_ON_DATASET_FAILURE:-true}
    MAX_DATASET_FAILURES=${MAX_DATASET_FAILURES:-0}
    
    vlog "Konfiguration geladen:"
    vlog "- DATASETS: $DATASETS"
    vlog "- BACKUP_POOL: $BACKUP_POOL"
    vlog "- MIN_FREE_SPACE_GB: $MIN_FREE_SPACE_GB"
    vlog "- CONTINUE_ON_DATASET_FAILURE: $CONTINUE_ON_DATASET_FAILURE"
    vlog "- MAX_DISK_USAGE_HOURS: $MAX_DISK_USAGE_HOURS"
}

# Lockfile prüfen und erstellen
acquire_lock() {
    if [[ -f "$LOCKFILE" ]]; then
        local lock_age=$(($(date +%s) - $(stat -c %Y "$LOCKFILE")))
        if [[ $lock_age -gt ${MAX_LOCK_AGE:-3600} ]]; then
            log "WARN" "Verwaistes Lockfile gefunden (Alter: ${lock_age}s), wird entfernt"
            rm -f "$LOCKFILE"
        else
            error_exit "Backup bereits aktiv (Lockfile: $LOCKFILE)"
        fi
    fi
    
    echo $$ > "$LOCKFILE"
    vlog "Lockfile erstellt: $LOCKFILE"
}

# Disk-Rotation Verzeichnis erstellen
ensure_disk_rotation_dir() {
    if [[ ! -d "$DISK_ROTATION_DIR" ]]; then
        mkdir -p "$DISK_ROTATION_DIR"
        vlog "Disk-Rotation Verzeichnis erstellt: $DISK_ROTATION_DIR"
    fi
}

# Disk-Nutzung tracken
track_disk_usage() {
    local disk_id="$1"
    local action="$2"  # "start" oder "end"
    
    local tracking_file="$DISK_ROTATION_DIR/${disk_id}.usage"
    local timestamp=$(date +%s)
    
    case "$action" in
        "start")
            echo "start_time=$timestamp" > "$tracking_file"
            echo "disk_id=$disk_id" >> "$tracking_file"
            vlog "Disk-Nutzung gestartet: $disk_id"
            ;;
        "end")
            if [[ -f "$tracking_file" ]]; then
                local start_time=$(grep "^start_time=" "$tracking_file" | cut -d= -f2)
                local duration_hours=$(( (timestamp - start_time) / 3600 ))
                echo "end_time=$timestamp" >> "$tracking_file"
                echo "duration_hours=$duration_hours" >> "$tracking_file"
                vlog "Disk-Nutzung beendet: $disk_id (Dauer: ${duration_hours}h)"
            fi
            ;;
    esac
}

# Disk-Nutzungsdauer prüfen
check_disk_usage_duration() {
    local disk_id="$1"
    
    if [[ "$MAX_DISK_USAGE_HOURS" -eq 0 ]]; then
        return 0  # Tracking deaktiviert
    fi
    
    local tracking_file="$DISK_ROTATION_DIR/${disk_id}.usage"
    
    if [[ ! -f "$tracking_file" ]]; then
        return 0  # Neue Disk
    fi
    
    # Prüfe ob Disk noch in Verwendung ist
    if ! grep -q "^end_time=" "$tracking_file"; then
        # Disk ist noch in Verwendung, prüfe Dauer
        local start_time=$(grep "^start_time=" "$tracking_file" | cut -d= -f2 2>/dev/null || echo "0")
        local current_time=$(date +%s)
        local usage_hours=$(( (current_time - start_time) / 3600 ))
        
        if [[ $usage_hours -gt $MAX_DISK_USAGE_HOURS ]]; then
            log "WARN" "Disk $disk_id wird seit ${usage_hours}h verwendet (Maximum: ${MAX_DISK_USAGE_HOURS}h)"
            return 1
        fi
    fi
    
    return 0
}

# Disk-Identifikation mit Fallback-Strategien
identify_disk_robust() {
    local disk_id="$1"
    local timeout_end=$(($(date +%s) + DISK_DETECTION_TIMEOUT))
    
    vlog "Suche Disk mit ID: $disk_id (Timeout: ${DISK_DETECTION_TIMEOUT}s)"
    
    while [[ $(date +%s) -lt $timeout_end ]]; do
        local disk_path=""
        
        # Strategie 1: WWN-basierte Erkennung
        if [[ -e "/dev/disk/by-id/wwn-$disk_id" ]]; then
            disk_path="/dev/disk/by-id/wwn-$disk_id"
            vlog "Disk gefunden über WWN: $disk_path"
        
        # Strategie 2: ATA-Serial-basierte Erkennung
        elif ls /dev/disk/by-id/ata-*$disk_id* >/dev/null 2>&1; then
            disk_path=$(ls /dev/disk/by-id/ata-*$disk_id* | head -1)
            vlog "Disk gefunden über ATA-Serial: $disk_path"
        
        # Strategie 3: SCSI-ID-basierte Erkennung
        elif ls /dev/disk/by-id/scsi-*$disk_id* >/dev/null 2>&1; then
            disk_path=$(ls /dev/disk/by-id/scsi-*$disk_id* | head -1)
            vlog "Disk gefunden über SCSI-ID: $disk_path"
        
        # Strategie 4: USB-Serial (für USB-Disks)
        elif ls /dev/disk/by-id/usb-*$disk_id* >/dev/null 2>&1; then
            disk_path=$(ls /dev/disk/by-id/usb-*$disk_id* | head -1)
            vlog "Disk gefunden über USB-Serial: $disk_path"
        
        # Fallback-Strategie: Alle verfügbaren Disks scannen
        elif [[ "$ENABLE_DISK_FALLBACK" == true ]]; then
            vlog "Fallback: Scanne alle verfügbaren Block-Geräte..."
            for dev in /dev/disk/by-id/*; do
                if [[ -e "$dev" ]] && [[ "$dev" == *"$disk_id"* ]]; then
                    disk_path="$dev"
                    vlog "Disk gefunden über Fallback-Scan: $disk_path"
                    break
                fi
            done
        fi
        
        if [[ -n "$disk_path" ]]; then
            # Prüfe ob Disk tatsächlich verfügbar ist
            if [[ -b "$disk_path" ]]; then
                echo "$disk_path"
                return 0
            else
                vlog "Disk-Pfad existiert, aber ist kein Block-Gerät: $disk_path"
            fi
        fi
        
        sleep 1
    done
    
    return 1
}

# Backup-Pool mit Retry-Logik importieren
import_backup_pool_robust() {
    local disk_path="$1"
    local disk_dir="$(dirname "$disk_path")"
    local retry_count=0
    
    while [[ $retry_count -lt $DISK_IMPORT_RETRIES ]]; do
        log "INFO" "Importiere Backup-Pool: $BACKUP_POOL (Versuch $((retry_count + 1))/$DISK_IMPORT_RETRIES)"
        
        if [[ "$DRY_RUN" == false ]]; then
            if zpool import -d "$disk_dir" "$BACKUP_POOL" 2>/dev/null; then
                log "INFO" "Backup-Pool erfolgreich importiert"
                return 0
            else
                log "WARN" "Import-Versuch $((retry_count + 1)) fehlgeschlagen"
                retry_count=$((retry_count + 1))
                
                if [[ $retry_count -lt $DISK_IMPORT_RETRIES ]]; then
                    log "INFO" "Warte ${DISK_IMPORT_RETRY_DELAY}s vor nächstem Versuch..."
                    sleep "$DISK_IMPORT_RETRY_DELAY"
                fi
            fi
        else
            log "INFO" "Backup-Pool Import (Dry-Run)"
            return 0
        fi
    done
    
    return 1
}

# Backup-Festplatte identifizieren und importieren
identify_and_import_backup_disk() {
    log "INFO" "Suche nach Backup-Festplatte..."
    
    ensure_disk_rotation_dir
    
    local found_disk=""
    local found_disk_id=""
    IFS=',' read -ra DISK_IDS <<< "$BACKUP_DISK_IDS"
    
    # Priorisierte Suche: Erste verfügbare Disk aus der Liste verwenden
    for disk_id in "${DISK_IDS[@]}"; do
        disk_id=$(echo "$disk_id" | xargs) # Whitespace entfernen
        
        local disk_path=$(identify_disk_robust "$disk_id")
        
        if [[ -n "$disk_path" ]]; then
            found_disk="$disk_path"
            found_disk_id="$disk_id"
            log "INFO" "Backup-Festplatte gefunden: $found_disk (ID: $disk_id)"
            
            # Prüfe Disk-Nutzungsdauer
            if ! check_disk_usage_duration "$disk_id"; then
                log "WARN" "Disk-Rotation Warnung für $disk_id - sollte gewechselt werden"
            fi
            
            break
        else
            vlog "Disk nicht gefunden: $disk_id"
        fi
    done
    
    if [[ -z "$found_disk" ]]; then
        error_exit "Keine gültige Backup-Festplatte gefunden. Erwartete IDs: $BACKUP_DISK_IDS"
    fi
    
    # Globale Variablen setzen
    CURRENT_DISK_ID="$found_disk_id"
    CURRENT_DISK_PATH="$found_disk"
    
    # Disk-Nutzung tracken
    track_disk_usage "$found_disk_id" "start"
    
    # Prüfen ob Pool bereits importiert ist
    if zpool list "$BACKUP_POOL" >/dev/null 2>&1; then
        log "INFO" "Backup-Pool ist bereits importiert: $BACKUP_POOL"
        return
    fi
    
    # Pool mit Retry-Logik importieren
    if ! import_backup_pool_robust "$found_disk"; then
        error_exit "Fehler beim Import des Backup-Pools nach $DISK_IMPORT_RETRIES Versuchen"
    fi
}

# Speicherplatz prüfen
check_free_space() {
    log "INFO" "Prüfe verfügbaren Speicherplatz auf $BACKUP_POOL"
    
    local available_gb
    if [[ "$DRY_RUN" == false ]]; then
        available_gb=$(zfs list -H -o avail "$BACKUP_POOL" | numfmt --from=iec --to-unit=1G)
    else
        available_gb=100 # Dummy-Wert für Dry-Run
    fi
    
    if (( $(echo "$available_gb < $MIN_FREE_SPACE_GB" | bc -l) )); then
        error_exit "Zu wenig freier Speicherplatz: ${available_gb}GB < ${MIN_FREE_SPACE_GB}GB"
    fi
    
    log "INFO" "Verfügbarer Speicherplatz: ${available_gb}GB (Minimum: ${MIN_FREE_SPACE_GB}GB)"
}

# Dataset-Mapping parsen
parse_dataset_mapping() {
    local dataset_config="$1"
    local source_dataset=""
    local target_dataset=""
    
    if [[ "$dataset_config" == *":"* ]]; then
        # Format: "source:target"
        source_dataset="${dataset_config%%:*}"
        target_dataset="${dataset_config##*:}"
        
        # Wenn target_dataset nicht mit dem BACKUP_POOL beginnt, ist es relativ
        if [[ "$target_dataset" != "$BACKUP_POOL"* ]]; then
            target_dataset="${BACKUP_POOL}/${target_dataset}"
        fi
    else
        # Traditionelles Format: "rpool/data" -> "backuppool/rpool/data"
        source_dataset="$dataset_config"
        target_dataset="${BACKUP_POOL}/${source_dataset}"
    fi
    
    echo "$source_dataset|$target_dataset"
}

# Gemeinsamen Snapshot finden
find_common_snapshot() {
    local source_dataset="$1"
    local target_dataset="$2"
    
    vlog "Suche gemeinsamen Snapshot zwischen $source_dataset und $target_dataset" >&2
    
    # Prüfe zuerst ob das Ziel-Dataset überhaupt existiert
    if [[ "$DRY_RUN" == false ]]; then
        if ! zfs list "$target_dataset" >/dev/null 2>&1; then
            vlog "Ziel-Dataset $target_dataset existiert nicht - kein gemeinsamer Snapshot möglich" >&2
            echo ""
            return
        fi
    fi
    
    # Alle zfs-auto-snap_daily Snapshots der Quelle holen
    local source_snapshots
    if [[ "$DRY_RUN" == false ]]; then
        source_snapshots=$(zfs list -H -t snapshot -o name "$source_dataset" 2>/dev/null | grep "${SNAPSHOT_PREFIX:-zfs-auto-snap_daily}" | sort -r || true)
    else
        source_snapshots="$source_dataset@zfs-auto-snap_daily-2024-01-20-00h00"
    fi
    
    if [[ -z "$source_snapshots" ]]; then
        vlog "Keine ${SNAPSHOT_PREFIX:-zfs-auto-snap_daily} Snapshots in $source_dataset gefunden" >&2
        echo ""
        return
    fi
    
    # Alle Snapshots des Ziels holen
    local target_snapshots
    if [[ "$DRY_RUN" == false ]]; then
        target_snapshots=$(zfs list -H -t snapshot -o name "$target_dataset" 2>/dev/null | grep "${SNAPSHOT_PREFIX:-zfs-auto-snap_daily}" || true)
    else
        target_snapshots="$target_dataset@zfs-auto-snap_daily-2024-01-20-00h00"
    fi
    
    if [[ -z "$target_snapshots" ]]; then
        vlog "Keine ${SNAPSHOT_PREFIX:-zfs-auto-snap_daily} Snapshots in $target_dataset gefunden" >&2
        echo ""
        return
    fi
    
    # Prüfe jeden Quell-Snapshot auf Existenz im Ziel
    while IFS= read -r source_snapshot; do
        [[ -z "$source_snapshot" ]] && continue
        
        local snap_name="${source_snapshot##*@}"
        local target_snapshot="${target_dataset}@${snap_name}"
        
        # Prüfe ob entsprechender Snapshot im Ziel existiert
        if echo "$target_snapshots" | grep -q "^${target_snapshot}$"; then
            vlog "Gemeinsamer Snapshot gefunden: $source_snapshot" >&2
            echo "$source_snapshot"
            return
        fi
    done <<< "$source_snapshots"
    
    vlog "Kein gemeinsamer Snapshot gefunden" >&2
    echo ""
}

# Neuesten Snapshot finden
find_latest_snapshot() {
    local dataset="$1"
    
    if [[ "$DRY_RUN" == false ]]; then
        zfs list -H -t snapshot -o name "$dataset" 2>/dev/null | grep "${SNAPSHOT_PREFIX:-zfs-auto-snap_daily}" | sort -r | head -1 || true
    else
        echo "$dataset@zfs-auto-snap_daily-2024-01-21-00h00"
    fi
}

# Snapshot übertragen
transfer_snapshot() {
    local source_dataset="$1"
    local target_dataset="$2"
    local common_snapshot="$3"
    local latest_snapshot="$4"
    
    log "INFO" "Übertrage Snapshot: $latest_snapshot"
    
    if [[ -n "$common_snapshot" ]]; then
        # Inkrementelle Übertragung
        log "INFO" "Inkrementelle Übertragung von $common_snapshot zu $latest_snapshot"
        vlog "Befehl: zfs send -I $common_snapshot $latest_snapshot | zfs receive -F $target_dataset"
        
        if [[ "$DRY_RUN" == false ]]; then
            if ! zfs send -I "$common_snapshot" "$latest_snapshot" | zfs receive -F "$target_dataset"; then
                log "ERROR" "Fehler bei inkrementeller Übertragung"
                return 1
            fi
        fi
    else
        # Vollständige Übertragung
        if [[ "$FULL_BACKUP_ON_NO_COMMON" == true ]]; then
            log "INFO" "Vollständige Übertragung: $latest_snapshot"
            vlog "Befehl: zfs send $latest_snapshot | zfs receive -F $target_dataset"
            
            if [[ "$DRY_RUN" == false ]]; then
                if ! zfs send "$latest_snapshot" | zfs receive -F "$target_dataset"; then
                    log "ERROR" "Fehler bei vollständiger Übertragung"
                    return 1
                fi
            fi
        else
            log "WARN" "Kein gemeinsamer Snapshot gefunden und FULL_BACKUP_ON_NO_COMMON=false"
            return 1
        fi
    fi
    
    return 0
}

# Alte Snapshots bereinigen (alle zfs-auto-snap Typen)
cleanup_old_snapshots() {
    local source_dataset="$1"
    local target_dataset="$2"
    
    log "INFO" "Bereinige alte Snapshots in $target_dataset"
    
    # Alle zfs-auto-snap Snapshot-Typen definieren
    local snap_types=("daily" "hourly" "weekly" "monthly" "frequent")
    
    for snap_type in "${snap_types[@]}"; do
        vlog "Bereinige $snap_type Snapshots..."
        
        # Alle zfs-auto-snap_$snap_type Snapshots der Quelle holen
        local source_snapshots
        if [[ "$DRY_RUN" == false ]]; then
            source_snapshots=$(zfs list -H -t snapshot -o name "$source_dataset" 2>/dev/null | grep "zfs-auto-snap_${snap_type}" | sed "s/.*@//" || true)
        else
            source_snapshots="zfs-auto-snap_${snap_type}-2024-01-21-00h00"
        fi
        
        # Alle zfs-auto-snap_$snap_type Snapshots des Ziels holen
        local target_snapshots
        if [[ "$DRY_RUN" == false ]]; then
            target_snapshots=$(zfs list -H -t snapshot -o name "$target_dataset" 2>/dev/null | grep "zfs-auto-snap_${snap_type}" || true)
        else
            target_snapshots="$target_dataset@zfs-auto-snap_${snap_type}-2024-01-20-00h00"
        fi
        
        if [[ -z "$target_snapshots" ]]; then
            vlog "Keine $snap_type Snapshots im Ziel gefunden"
            continue
        fi
        
        # Prüfe jeden Ziel-Snapshot
        while IFS= read -r target_snapshot; do
            [[ -z "$target_snapshot" ]] && continue
            
            local snap_name="${target_snapshot##*@}"
            
            # Prüfe ob Snapshot noch in Quelle existiert
            if ! echo "$source_snapshots" | grep -q "^$snap_name$"; then
                log "INFO" "Lösche verwaisten $snap_type Snapshot: $target_snapshot"
                if [[ "$DRY_RUN" == false ]]; then
                    zfs destroy "$target_snapshot" || log "WARN" "Fehler beim Löschen von $target_snapshot"
                fi
            else
                vlog "Behalte $snap_type Snapshot: $target_snapshot (existiert noch in Quelle)"
            fi
        done <<< "$target_snapshots"
    done
}

# CheckMK Piggyback-Datei schreiben (Global)
write_checkmk_piggyback_global() {
    local overall_status="$1"
    local disk_id="$2"
    local disk_usage_hours="$3"
    
    if [[ "$CHECKMK_ENABLED" != true ]]; then
        return
    fi
    
    local hostname=$(hostname)
    local piggyback_dir="$CHECKMK_PIGGYBACK_DIR/$hostname"
    local piggyback_file="$piggyback_dir/zfs_backup_global"
    
    # Verzeichnis erstellen falls nicht vorhanden
    mkdir -p "$piggyback_dir"
    
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    cat > "$piggyback_file" << EOF
<<<zfs_backup_global>>>
Status: $overall_status
Timestamp: $timestamp
Success_Count: $BACKUP_SUCCESS_COUNT
Failure_Count: $BACKUP_FAILURE_COUNT
Current_Disk: $disk_id
Disk_Usage_Hours: $disk_usage_hours
Max_Disk_Usage_Hours: $MAX_DISK_USAGE_HOURS
EOF
    
    # Disk-Rotation Warnung
    if [[ "$MAX_DISK_USAGE_HOURS" -gt 0 ]] && [[ "$disk_usage_hours" -gt "$MAX_DISK_USAGE_HOURS" ]]; then
        echo "Disk_Rotation_Warning: true" >> "$piggyback_file"
    else
        echo "Disk_Rotation_Warning: false" >> "$piggyback_file"
    fi
    
    vlog "CheckMK Global Piggyback-Datei geschrieben: $piggyback_file"
}

# CheckMK Piggyback-Datei schreiben (Dataset-spezifisch)
write_checkmk_piggyback_dataset() {
    local dataset_name="$1"
    local status="$2"
    local snapshot_name="$3"
    local snapshot_guid="$4"
    local backup_type="$5"
    
    if [[ "$CHECKMK_ENABLED" != true ]] || [[ "$CHECKMK_DETAILED_SERVICES" != true ]]; then
        return
    fi
    
    local hostname=$(hostname)
    local piggyback_dir="$CHECKMK_PIGGYBACK_DIR/$hostname"
    local safe_dataset_name=$(echo "$dataset_name" | tr '/' '_')
    local piggyback_file="$piggyback_dir/zfs_backup_${safe_dataset_name}"
    
    # Verzeichnis erstellen falls nicht vorhanden
    mkdir -p "$piggyback_dir"
    
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    cat > "$piggyback_file" << EOF
<<<zfs_backup_dataset>>>
Dataset: $dataset_name
Status: $status
Timestamp: $timestamp
Snapshot: $snapshot_name
GUID: $snapshot_guid
Backup_Type: $backup_type
EOF
    
    vlog "CheckMK Dataset Piggyback-Datei geschrieben: $piggyback_file"
}

# System-Update durchführen
perform_system_update() {
    if [[ "$SYSTEM_UPDATE_ENABLED" != true ]]; then
        return
    fi
    
    log "INFO" "Führe System-Update durch..."
    
    if [[ "$DRY_RUN" == false ]]; then
        if apt update && apt dist-upgrade -y; then
            log "INFO" "System-Update erfolgreich abgeschlossen"
        else
            log "WARN" "System-Update fehlgeschlagen"
        fi
    else
        log "INFO" "System-Update (Dry-Run)"
    fi
}

# Hauptfunktion für Dataset-Backup
backup_dataset() {
    local dataset_config="$1"
    
    # Dataset-Mapping parsen
    local mapping=$(parse_dataset_mapping "$dataset_config")
    local source_dataset="${mapping%%|*}"
    local target_dataset="${mapping##*|}"
    
    log "INFO" "Starte Backup für Dataset: $source_dataset -> $target_dataset"
    
    # Prüfe ob Quell-Dataset existiert
    if [[ "$DRY_RUN" == false ]]; then
        if ! zfs list "$source_dataset" >/dev/null 2>&1; then
            log "ERROR" "Quell-Dataset existiert nicht: $source_dataset"
            write_checkmk_piggyback_dataset "$source_dataset" "FAIL" "N/A" "unknown" "N/A"
            return 1
        fi
    fi
    
    # Prüfe ob Ziel-Dataset existiert
    local target_exists=false
    if [[ "$DRY_RUN" == false ]]; then
        if zfs list "$target_dataset" >/dev/null 2>&1; then
            target_exists=true
            log "INFO" "Ziel-Dataset existiert bereits: $target_dataset"
        else
            log "INFO" "Ziel-Dataset existiert nicht: $target_dataset"
        fi
    else
        log "INFO" "Ziel-Dataset würde geprüft werden (Dry-Run): $target_dataset"
        target_exists=true # Für Dry-Run annehmen dass es existiert
    fi
    
    # Neuesten Snapshot finden
    local latest_snapshot=$(find_latest_snapshot "$source_dataset")
    
    if [[ -z "$latest_snapshot" ]]; then
        log "WARN" "Kein ${SNAPSHOT_PREFIX:-zfs-auto-snap_daily} Snapshot in $source_dataset gefunden"
        write_checkmk_piggyback_dataset "$source_dataset" "FAIL" "N/A" "unknown" "N/A"
        return 1
    fi
    
    # Gemeinsamen Snapshot nur suchen wenn Ziel bereits existiert
    local common_snapshot=""
    local backup_type="full"
    if [[ "$target_exists" == true ]]; then
        common_snapshot=$(find_common_snapshot "$source_dataset" "$target_dataset")
        if [[ -n "$common_snapshot" ]]; then
            backup_type="incremental"
        fi
    fi
    
    vlog "Gemeinsamer Snapshot: ${common_snapshot:-'keiner'}"
    vlog "Neuester Snapshot: $latest_snapshot"
    vlog "Backup-Typ: $backup_type"
    
    # Entscheidung über Backup-Typ
    if [[ -n "$common_snapshot" ]]; then
        log "INFO" "Führe inkrementelles Backup durch (gemeinsamer Snapshot: ${common_snapshot##*@})"
    elif [[ "$target_exists" == false ]] || [[ "$FULL_BACKUP_ON_NO_COMMON" == true ]]; then
        if [[ "$target_exists" == false ]]; then
            log "INFO" "Führe initiales vollständiges Backup durch (neues Ziel-Dataset)"
        else
            log "INFO" "Führe vollständiges Backup durch (FULL_BACKUP_ON_NO_COMMON=true)"
        fi
        common_snapshot="" # Sicherstellen dass vollständiges Backup gemacht wird
    else
        log "WARN" "Kein gemeinsamer Snapshot gefunden und FULL_BACKUP_ON_NO_COMMON=false"
        log "INFO" "Optionen:"
        log "INFO" "  1. Setze FULL_BACKUP_ON_NO_COMMON=true in der Konfiguration"
        log "INFO" "  2. Führe manuelles vollständiges Backup durch:"
        log "INFO" "     zfs send $latest_snapshot | zfs receive -F $target_dataset"
        write_checkmk_piggyback_dataset "$source_dataset" "FAIL" "$latest_snapshot" "unknown" "skipped"
        return 1
    fi
    
    # Snapshot übertragen
    if transfer_snapshot "$source_dataset" "$target_dataset" "$common_snapshot" "$latest_snapshot"; then
        # Alte Snapshots bereinigen
        cleanup_old_snapshots "$source_dataset" "$target_dataset"
        
        # Snapshot-GUID für CheckMK holen
        local snapshot_guid=""
        if [[ "$DRY_RUN" == false ]]; then
            snapshot_guid=$(zfs get -H -o value guid "$latest_snapshot" 2>/dev/null || echo "unknown")
        else
            snapshot_guid="12345678-1234-1234-1234-123456789abc"
        fi
        
        write_checkmk_piggyback_dataset "$source_dataset" "OK" "$latest_snapshot" "$snapshot_guid" "$backup_type"
        log "INFO" "Backup für $source_dataset erfolgreich abgeschlossen"
        return 0
    else
        write_checkmk_piggyback_dataset "$source_dataset" "FAIL" "$latest_snapshot" "unknown" "$backup_type"
        log "ERROR" "Backup für $source_dataset fehlgeschlagen"
        return 1
    fi
}

# Berechne aktuelle Disk-Nutzungsdauer
get_current_disk_usage_hours() {
    local disk_id="$1"
    
    if [[ -z "$disk_id" ]]; then
        echo "0"
        return
    fi
    
    local tracking_file="$DISK_ROTATION_DIR/${disk_id}.usage"
    
    if [[ ! -f "$tracking_file" ]]; then
        echo "0"
        return
    fi
    
    # Prüfe ob Disk noch in Verwendung ist
    if ! grep -q "^end_time=" "$tracking_file"; then
        # Disk ist noch in Verwendung, berechne aktuelle Dauer
        local start_time=$(grep "^start_time=" "$tracking_file" | cut -d= -f2 2>/dev/null || echo "0")
        local current_time=$(date +%s)
        local usage_hours=$(( (current_time - start_time) / 3600 ))
        echo "$usage_hours"
    else
        # Disk ist nicht mehr in Verwendung
        local duration_hours=$(grep "^duration_hours=" "$tracking_file" | cut -d= -f2 2>/dev/null || echo "0")
        echo "$duration_hours"
    fi
}

# Hauptfunktion
main() {
    local start_time=$(date)
    log "INFO" "=== ZFS Backup v2.0 gestartet ==="
    log "INFO" "Startzeit: $start_time"
    
    # Konfiguration laden
    load_config
    
    # Lockfile erstellen
    acquire_lock
    
    # Backup-Festplatte identifizieren und importieren
    identify_and_import_backup_disk
    
    # Speicherplatz prüfen
    check_free_space
    
    # Datasets sichern mit flexiblem Mapping und Graceful Degradation
    local overall_success=true
    IFS=',' read -ra DATASET_ARRAY <<< "$DATASETS"
    
    for dataset_config in "${DATASET_ARRAY[@]}"; do
        dataset_config=$(echo "$dataset_config" | xargs) # Whitespace entfernen
        
        if backup_dataset "$dataset_config"; then
            BACKUP_SUCCESS_COUNT=$((BACKUP_SUCCESS_COUNT + 1))
            log "INFO" "Dataset erfolgreich gesichert: $dataset_config"
        else
            BACKUP_FAILURE_COUNT=$((BACKUP_FAILURE_COUNT + 1))
            log "ERROR" "Dataset-Backup fehlgeschlagen: $dataset_config"
            
            # Prüfe ob weitergemacht werden soll
            if [[ "$CONTINUE_ON_DATASET_FAILURE" != true ]]; then
                log "ERROR" "CONTINUE_ON_DATASET_FAILURE=false - Backup wird abgebrochen"
                overall_success=false
                break
            fi
            
            # Prüfe maximale Anzahl Fehler
            if [[ "$MAX_DATASET_FAILURES" -gt 0 ]] && [[ "$BACKUP_FAILURE_COUNT" -ge "$MAX_DATASET_FAILURES" ]]; then
                log "ERROR" "Maximale Anzahl Dataset-Fehler erreicht ($MAX_DATASET_FAILURES) - Backup wird abgebrochen"
                overall_success=false
                break
            fi
            
            overall_success=false
        fi
    done
    
    # Disk-Nutzung beenden tracken
    if [[ -n "$CURRENT_DISK_ID" ]]; then
        track_disk_usage "$CURRENT_DISK_ID" "end"
    fi
    
    # Aktuelle Disk-Nutzungsdauer ermitteln
    local current_disk_usage_hours=$(get_current_disk_usage_hours "$CURRENT_DISK_ID")
    
    # Gesamtstatus bestimmen
    local overall_status="OK"
    if [[ "$overall_success" != true ]]; then
        if [[ "$BACKUP_SUCCESS_COUNT" -gt 0 ]]; then
            overall_status="PARTIAL"
        else
            overall_status="FAIL"
        fi
    fi
    
    # CheckMK Global Status schreiben
    write_checkmk_piggyback_global "$overall_status" "$CURRENT_DISK_ID" "$current_disk_usage_hours"
    
    # System-Update (vor Export, nur bei erfolgreichem Backup)
    if [[ "$overall_status" == "OK" ]]; then
        perform_system_update
    fi
    
    # Backup-Pool exportieren (wird durch cleanup() erledigt)
    log "INFO" "Backup-Prozess abgeschlossen"
    log "INFO" "Erfolgreiche Datasets: $BACKUP_SUCCESS_COUNT"
    log "INFO" "Fehlgeschlagene Datasets: $BACKUP_FAILURE_COUNT"
    log "INFO" "Gesamtstatus: $overall_status"
    
    local end_time=$(date)
    log "INFO" "Endzeit: $end_time"
    log "INFO" "=== ZFS Backup v2.0 beendet ==="
    
    if [[ "$overall_status" == "FAIL" ]]; then
        exit 1
    elif [[ "$overall_status" == "PARTIAL" ]]; then
        exit 2
    fi
}

# Parameter-Parsing
while getopts "vd" opt; do
    case $opt in
        v)
            VERBOSE=true
            ;;
        d)
            DRY_RUN=true
            log "INFO" "Dry-Run Modus aktiviert"
            ;;
        \?)
            echo "Verwendung: $0 [-v] [-d]"
            echo "  -v  Verbose Logging"
            echo "  -d  Dry-Run (nur Simulation)"
            exit 1
            ;;
    esac
done

# Skript als Root ausführen
if [[ $EUID -ne 0 ]]; then
    error_exit "Dieses Skript muss als Root ausgeführt werden"
fi

# Abhängigkeiten prüfen
for cmd in zfs zpool bc numfmt; do
    if ! command -v "$cmd" >/dev/null 2>&1; then
        error_exit "Erforderliches Programm nicht gefunden: $cmd"
    fi
done

# Hauptfunktion ausführen
main "$@"
